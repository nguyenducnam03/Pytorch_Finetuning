# Fine-Tuning BERT, GPT-2, and T5 using PyTorch

## Overview
This repository explores fine-tuning transformer-based models (BERT, GPT-2, and T5) (Encoder-only, Decoder-only, Encoder-Decoder model) using PyTorch. The research is divided into two main approaches:

- **BERT**: Understanding the model architecture and fine-tuning it for sentiment analysis.
- **GPT-2 & T5**: Directly fine-tuning from pre-trained models without diving into architecture details, focusing on specific tasks.
  - **GPT-2**: Fine-tuning for text generation.
  - **T5**: Fine-tuning for translation tasks.

## Models and Fine-Tuning Tasks
| Model  | Task  | Approach |
|--------|-------|----------|
| BERT   | Sentiment Analysis | Understanding architecture + Fine-tuning |
| GPT-2  | Text Generation  | Fine-tuning from pre-trained model |
| T5     | Translation  | Fine-tuning from pre-trained model |

